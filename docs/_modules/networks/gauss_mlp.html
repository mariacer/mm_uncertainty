
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>networks.gauss_mlp &#8212; regression_uncertainty_mm 0.1 documentation</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for networks.gauss_mlp</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/env python3</span>
<span class="c1"># Copyright 2020 Christian Henning</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>
<span class="c1"># @title          :probabilistic/gauss_mlp.py</span>
<span class="c1"># @author         :ch</span>
<span class="c1"># @contact        :henningc@ethz.ch</span>
<span class="c1"># @created        :01/13/2020</span>
<span class="c1"># @version        :1.0</span>
<span class="c1"># @python_version :3.6.9</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">MLP that implements the local reparametrization trick</span>
<span class="sd">-----------------------------------------------------</span>

<span class="sd">The module :mod:`probabilisic.gauss_mlp` contains a MLP reimplementation that</span>
<span class="sd">implements the local reparametrization trick as described in</span>

<span class="sd">    Kingma et al., &quot;Variational Dropout and the Local Reparameterization Trick&quot;,</span>
<span class="sd">    2015. https://arxiv.org/abs/1506.02557</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">hypnettorch.mnets</span> <span class="k">import</span> <span class="n">MLP</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="k">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="k">import</span> <span class="n">Normal</span>

<span class="kn">from</span> <span class="nn">probabilistic</span> <span class="k">import</span> <span class="n">prob_utils</span> <span class="k">as</span> <span class="n">putils</span>

<div class="viewcode-block" id="GaussianMLP"><a class="viewcode-back" href="../../networks.html#networks.gauss_mlp.GaussianMLP">[docs]</a><span class="k">class</span> <span class="nc">GaussianMLP</span><span class="p">(</span><span class="n">MLP</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Multi-layer-perceptron with fully-factorized Gaussian weight posterior.</span>

<span class="sd">    This class assumes that the weight posterior is a fully-factorized Gaussian,</span>
<span class="sd">    such that the local reparametrization trick can be applied. Hence, assuming</span>
<span class="sd">    a batch size :math:`M`, a layer input size of :math:`N_{in}` and a layer</span>
<span class="sd">    output size of :math:`N_{out}` a linear layer performs the following</span>
<span class="sd">    operation</span>

<span class="sd">    .. math::</span>

<span class="sd">        Y = X W</span>

<span class="sd">    with inputs :math:`X \in \mathbb{R}^{M \times N_{in}}`, weights</span>
<span class="sd">    :math:`W \in \mathbb{R}^{N_{in} \times N_{out}}` and outputs</span>
<span class="sd">    :math:`Y \in \mathbb{R}^{M \times N_{out}}`.</span>

<span class="sd">    Elements in :math:`W` are expected to arise from the following factorized</span>
<span class="sd">    Gaussian</span>

<span class="sd">    .. math::</span>

<span class="sd">        q_{\phi}(w_{ij}) = \mathcal{N} (\mu_{ij}, \sigma_{ij}^2)</span>

<span class="sd">    As shown by `Kingma et al. &lt;https://arxiv.org/abs/1506.02557&gt;`__, the</span>
<span class="sd">    variance of the likelihood estimator will decrease to zero with increasing</span>
<span class="sd">    batch size if a different weight sample is drawn for every sample in the</span>
<span class="sd">    mini-batch.</span>

<span class="sd">    They show a more computational efficient solution is to sample activations,</span>
<span class="sd">    which follow the following factorized Gaussian given the above weight</span>
<span class="sd">    posterior</span>

<span class="sd">    .. math::</span>

<span class="sd">        q_{\phi}(y_{mj}) = \mathcal{N} (\gamma_{mj}, \delta_{mj}) \quad \</span>
<span class="sd">            \text{with} \quad \</span>
<span class="sd">            \gamma_{mj} = \sum_{i=1}^{N_{in}} x_{mi} \mu_{ij} \quad \</span>
<span class="sd">            \delta_{mj} = \sum_{i=1}^{N_{in}} x_{mi}^2 \sigma_{ij}^2</span>

<span class="sd">    Hence, we can simply sample activations using the above distribution and</span>
<span class="sd">    the reparametrization trick</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_{mj} = \gamma_{mj} + \sqrt{\delta_{mj}} \zeta_{mj} \quad \text{, } \</span>
<span class="sd">            \zeta_{mj} \sim \mathcal{N}(0, 1)</span>

<span class="sd">    What about bias vectors :math:`b \in \mathbb{R}^{N_{out}}`? We can easily</span>
<span class="sd">    incorporating them in the above formulas, by just assuming that we append an</span>
<span class="sd">    additional column to :math:`X` containing ones and an additional row to</span>
<span class="sd">    :math:`W` containing the bias vector. In this case, the formulas become</span>

<span class="sd">    .. math::</span>

<span class="sd">        \gamma_{mj} = \sum_{i=1}^{N_{in}} x_{mi} \mu_{ij} + \</span>
<span class="sd">            \hat{\mu}_{j} \quad \</span>
<span class="sd">            \delta_{mj} = \sum_{i=1}^{N_{in}} x_{mi}^2 \sigma_{ij}^2 + \</span>
<span class="sd">            \hat{\sigma}_{j}^2</span>

<span class="sd">    where we assume that</span>
<span class="sd">    :math:`q_{\phi}(b_{j}) = \mathcal{N} (\hat{\mu}_{j}, \hat{\sigma}_{j}^2)`.</span>

<span class="sd">    Note:</span>
<span class="sd">        This class will not hold mean and variance weights (even though, it</span>
<span class="sd">        is exoected that they are passed to the :meth:`forward`). Therefore, we</span>
<span class="sd">        recommend using an instance of this class always wrapped by class</span>
<span class="sd">        :class:`probabilistic.gauss_mnet_interface.GaussianBNNWrapper`.</span>

<span class="sd">    Args:</span>
<span class="sd">        (....): See constructor documentation of class :class:`mnets.mlp.MLP`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_in</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_out</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_layers</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
                 <span class="n">activation_fn</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">no_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">init_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="c1"># We don&#39;t need to change anything from the base class constructor,</span>
        <span class="c1"># we just need to modify the forward method.</span>
        <span class="n">MLP</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_in</span><span class="o">=</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="o">=</span><span class="n">n_out</span><span class="p">,</span> <span class="n">hidden_layers</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span>
                 <span class="n">activation_fn</span><span class="o">=</span><span class="n">activation_fn</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
                 <span class="n">no_weights</span><span class="o">=</span><span class="n">no_weights</span><span class="p">,</span> <span class="n">init_weights</span><span class="o">=</span><span class="n">init_weights</span><span class="p">,</span>
                 <span class="n">dropout_rate</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_spectral_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_context_mod</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">out_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>

<div class="viewcode-block" id="GaussianMLP.forward"><a class="viewcode-back" href="../../networks.html#networks.gauss_mlp.GaussianMLP.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="n">logvar_enc</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mean_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">sample</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">rand_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute the output :math:`y` of this network given the input</span>
<span class="sd">        :math:`x` by drawing a different weight sample for every sample in the</span>
<span class="sd">        input batch.</span>

<span class="sd">        This is achieved by using the local reparametrization trick.</span>

<span class="sd">        Args:</span>
<span class="sd">            (....): See docstring of method</span>
<span class="sd">                :meth:`mnets.mlp.MLP.forward`. We</span>
<span class="sd">                provide some more specific information below.</span>
<span class="sd">            mean (list): The set of mean parameters. The shapes of the tensors</span>
<span class="sd">                are expected to follow attribute</span>
<span class="sd">                :attr:`mnets.mnet_interface.MainNetInterface.param_shapes`.</span>
<span class="sd">            rho (list): The set of :math:`\rho` parameters. The shapes of the</span>
<span class="sd">                tensors are expected to follow attribute</span>
<span class="sd">                :attr:`mnets.mnet_interface.MainNetInterface.param_shapes`.</span>
<span class="sd">            logvar_enc (bool): See docstring of function</span>
<span class="sd">                :func:`probabilistic.prob_utils.decode_and_sample_diag_gauss`.</span>
<span class="sd">            mean_only (bool): If ``True``, the hidden activation will simply</span>
<span class="sd">                be set to :math:`\gamma`.</span>
<span class="sd">            sample (optional): If a specific weight sample is provided, the</span>
<span class="sd">                local raparametrization trick will be disabled and the super</span>
<span class="sd">                class ``forward`` method will be called where the sample is</span>
<span class="sd">                passed as argument ``weights``.</span>
<span class="sd">            rand_state (torch.Generator, optional): This generator would be used</span>
<span class="sd">                to realize the reparametrization trick (i.e., the activation</span>
<span class="sd">                sampling), if specified. This way, the behavior of the forward</span>
<span class="sd">                pass is easily reproducible.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The output of the network.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">sample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Do not use local reparam trick!</span>
            <span class="k">return</span> <span class="n">MLP</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">sample</span><span class="p">)</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_shapes</span><span class="p">)</span> <span class="ow">and</span> \
            <span class="nb">len</span><span class="p">(</span><span class="n">rho</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_shapes</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_shapes</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">mean</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)))</span> <span class="ow">and</span> \
                <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">rho</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)))</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">putils</span><span class="o">.</span><span class="n">decode_diag_gauss</span><span class="p">(</span><span class="n">rho</span><span class="p">,</span> <span class="n">return_var</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                          <span class="n">logvar_enc</span><span class="o">=</span><span class="n">logvar_enc</span><span class="p">)</span>

        <span class="n">w_means</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">b_means</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">w_vars</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">b_vars</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mean</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">b_means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
                <span class="n">b_vars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">var</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">w_means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
                <span class="n">w_vars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">var</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

        <span class="c1">###########################</span>
        <span class="c1">### Forward Computation ###</span>
        <span class="c1">###########################</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">x</span>

        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_means</span><span class="p">)):</span>
            <span class="n">mu</span> <span class="o">=</span> <span class="n">w_means</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
            <span class="n">sigma2</span> <span class="o">=</span> <span class="n">w_vars</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span><span class="p">:</span>
                <span class="n">mu_hat</span> <span class="o">=</span> <span class="n">b_means</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
                <span class="n">sigma2_hat</span> <span class="o">=</span> <span class="n">b_vars</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">mu_hat</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">sigma2_hat</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="c1"># Compute means and variances of activations.</span>
            <span class="n">gamma</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">mu_hat</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">mean_only</span><span class="p">:</span>
                <span class="n">hidden</span> <span class="o">=</span> <span class="n">gamma</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">delta</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">hidden</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">sigma2_hat</span><span class="p">)</span>

                <span class="c1"># Sample via reparametrization trick.</span>
                <span class="k">if</span> <span class="n">rand_state</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">hidden</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">delta</span><span class="o">.</span><span class="n">sqrt</span><span class="p">())</span><span class="o">.</span><span class="n">rsample</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">gamma</span><span class="p">),</span> <span class="mf">1.</span><span class="p">,</span>
                                       <span class="n">generator</span><span class="o">=</span><span class="n">rand_state</span><span class="p">)</span>
                    <span class="n">hidden</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">delta</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>

            <span class="c1"># Only for hidden layers.</span>
            <span class="k">if</span> <span class="n">l</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">w_means</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># Non-linearity</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_a_fun</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_a_fun</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hidden</span></div></div>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="k">pass</span>


</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">regression_uncertainty_mm</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents of this repository:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../gaussian_likelihoods.html">Gaussian Likelihoods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../norm_flow.html">Normalizing Flows</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../networks.html">Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../probabilistic.html">Probabilistic</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../datasets.html">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../utils.html">Utilities</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, Maria R. Cervera, Rafael Daetwyler, Hamza Keurti, Francesco dAngelo, Christian Henning.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    

    
  </body>
</html>